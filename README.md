# Signal-Processing
## Least mean squares filter
Least mean squares (LMS) algorithms are a class of adaptive filter used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean square of the error signal (difference between the desired and the actual signal). It is a stochastic gradient descent method in that the filter is only adapted based on the error at the current time. It was invented in 1960 by Stanford University professor Bernard Widrow and his first Ph.D. student, Ted Hoff.
##### ![image](https://user-images.githubusercontent.com/90008976/135154492-8ce205aa-c2d4-48c7-90b8-3d0367f351d7.png)
